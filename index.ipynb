{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828d5dba-fe46-45b3-a223-b546aba8b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa963fe-55b3-400b-88d4-f88281065a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import FAISS as fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec51afd-00d2-4a39-93f1-c37c1bd160ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your zip file\n",
    "zip_path = 'wayback.zip'\n",
    "# Directory to extract to\n",
    "extract_dir = 'wayback/'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777bf8b3-d692-4f92-8117-36cb7f5b1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in range(50,326):\n",
    "    filename = f\"wayback/wayback/wayback-{i}.txt\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename,\"r\") as infile:\n",
    "            content = infile.read()\n",
    "            docs.append(content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fdaa043-fc1b-4f52-8b88-1b4f1d9e89bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc62977-8cdd-43d8-959e-150c0795e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_article = 0\n",
    "for doc in docs:\n",
    "    words = doc.split(\" \")\n",
    "    len_doc = len(words)\n",
    "    if len_doc > max_article:\n",
    "        max_article = len_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be87539d-ff9f-417b-bb43-f7bc23641dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4721\n"
     ]
    }
   ],
   "source": [
    "print(max_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ddb1a2-15b5-4d69-8a64-0b52d30e3ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:184: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='microsoft/kosmos-2', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-8b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='yentinglin/llama-3-taiwan-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/paligemma', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='moonshotai/kimi-k2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/deplot', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-4-scout-17b-16e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvdev/meta/llama-4-maverick-17b-128e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvdev/meta/llama-4-scout-17b-16e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='adept/fuyu-8b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb7a2a0-d735-44df-b6b0-86fdf4c1526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter NVIDIA API KEY:  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\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Set NVIDIA API Key (replace with your actual key)\n",
    "os.environ[\"NVIDIA_API_KEY\"] = input(\"Enter NVIDIA API KEY: \")\n",
    "\n",
    "# STEP 2: Load the model via NVIDIA API\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
    "\n",
    "\n",
    "sys_msg0 = (\n",
    "    \"You are trying to assess whether an article from the user contains a strategy that might increase the user's cashflow. \"\n",
    "    \"You are only to respond with only one word. Yes if you think the article contains a strategy whether explicit or implicit that can be extract or no otherwise.\"\n",
    "    \n",
    ")\n",
    "sys_msg1 =(\n",
    "    \"You are a financial assistant. The user will provide an article that contain information explicitly or implicitly on improving cash flow.\"\n",
    "    \"Based strictly on the article’s content, state in under 150 words, how one can generate money using strategy implied in the article. Ensure the advice is general enough so it can be applied by any individual.\"\n",
    "    \"Do not mention any specific company names or details from the article. \"\n",
    "    \"Focus only on the strategy and how to implement it in a general context. Your response should be concise, practical, and universally applicable.\"\n",
    "    \"Ensure in your strategy description you state the product you will be offer, and the sustainable competitive advantage and the potential profit.\"\n",
    "    \n",
    ")\n",
    "sys_msg2 =(\n",
    "    \"You are a profiler. You are trying to assess the type of person that would implement the strategy provided by the user.\"\n",
    "    \"You are to state the age of the person, location(a country), highest education level(either: High School or less,Some College or Vocational Training, Bachelor's Degree, Master's or Professional Degree, Doctorate), \"\n",
    "    \"time investment(either: 1 - 3 hrs, 3 - 7 hrs, 7 - 20 hrs, 20 - 40 hrs, 40+ hrs)\"\n",
    "    \"capital required(either: < $200, $200 - $500, $500 - $2000, $2000 - $10000, $10000+)\"\n",
    "    \"time frame for return on investment(either: less than a month, 1 - 6 months, 6 - 12 months, 1 - 3 years, 3+ years)\"\n",
    "    \"Each aspect of the profile is to be on its own line. \"\n",
    "    \"E.g. Age: 40\\nLocation: Antigua\\nHighest Education Level: Master's or Profession Degree\\nTime Investment: 1 - 3 hrs\\nRequired Capital: $200 - $500\\nROI Time Frame: 1 - 6 months \"\n",
    "    \"No further information is to be include but the above. No need for explanation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0531d100-40d3-4a38-b8c9-e20d5fe17f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt0 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg0),    \n",
    "    (\"user\", \"[[{input}]]\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f01ef1-dc19-49f6-b46c-9bdbf97aa22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg1),    \n",
    "    (\"user\", \"[[{input}]]\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4130c721-999f-4290-8d3f-cd9bfa2cc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg2),    \n",
    "    (\"user\", \"[[{input}]]\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aca52fc-6e16-4b87-8c47-569d7aa09bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm0 = prompt0 | instruct_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2690f534-7ed8-44df-bd26-2e26cfcd1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = prompt1 | instruct_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e83a008-6f89-4633-a5f4-295059adda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = prompt2 | instruct_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7484024-de5e-4324-b411-0c83c7ff744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "print(llm0.invoke({'input':docs[0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91827458-27b1-479f-9698-4ada85affaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "finance_related = []\n",
    "for doc in docs:\n",
    "    ans = llm0.invoke({'input':doc})\n",
    "    if ans.lower().strip() != 'no':\n",
    "        finance_related.append(doc)\n",
    "        print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74bac7d4-2567-440a-9203-41adfa0acb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finance_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ba3c6a4-e3e3-4764-a4af-96d672d00351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you’ve been doing this job long enough, you start to develop strange interests (though some might compellingly argue that strange interests are a prerequisite). Lately for me it’s been power banks. Quite possibly the least sexy product in all of consumer electronics outside of the ever-ubiquitous dongle.\n",
      "\n",
      "I don’t know what to tell you. Blame the fact that I’m traveling every other week for this job. There are also all of the live blogs from years past that got cut off in the last few minutes as my poor, ancient MacBook put itself to sleep during those last precious battery percentages. Low batteries give me anxiety. I’m the guy who’s the first to notice when your phone’s screenshot is below 10 percent.\n",
      "\n",
      "So the power bank has become a constant accessory in my life, both at home and on the road. Until last year, I used to carry a massive one that was just north of 20,000 mAh. The peace of mind to back pain ration seemed sensible enough, but I learned the hard way that not only do Chinese airports have a limit on battery size, they chuck yours in the trash without a second thought if you go over. It’s a quick way to lose $150.\n",
      "\n",
      "The good news, however, is that between USB-C, wireless charging and the magic of crowdfunding, it seems we might be living through the golden age of the power bank. I know, right? What a time to be alive.\n",
      "\n",
      "Point is, there are a lot of choices out there. Anker and Amazon’s house brand RAVPower both offer some good options on a budget. There’s also mainstay Mophie for those who don’t mind paying a bit of a premium for design.\n",
      "\n",
      "Fuse Chicken was actually a brand that was new to me when they hit me up to try out their latest product. It’s a name I definitely would have remembered — because, honestly, it’s pretty terrible. Memorable, but terrible. Maybe that’s why the company went with such a mundane name for what’s a really interesting charger.\n",
      "\n",
      "My dad once told me that he gave my sister and I boring first names because we had such an unusual surname. I have no idea if this is true, but it’s an interesting story and could well apply here.\n",
      "\n",
      "The Universal is a good example of making the most out of a form factor. It manages to jam a lot of features in without creating a Frankenstein’s Monster worthy of the name Fuse Chicken. On its face, the product looks like a black and white version of Amazon’s default power bricks. It serves that purpose, of course, coupled with a trio of swappable international wall adapters (bonus points for travelers).\n",
      "\n",
      "But the brick also sports a 6,700 mAh battery inside, so you can continue charging gadgets while unplugged. That’s ideal for a phone — you can keep a laptop alive for a bit as well, but you’re going to burn through that pretty quickly. There’s also a wireless charging pad up top, so you can power up another phone or, say, a new set of AirPods at the same time. The side of the device features a small display showing how much juice is left.\n",
      "\n",
      "It’s great having a bank that’s also a plug, though like Apple’s brick, it’s much too massive to plug into many vertical outlets. I learned this lesson the hard way on a recent coast to coast flight. Thankfully, though, it’s compatible with Apple’s extension cable.\n",
      "\n",
      "OmniCharge, meanwhile, is a company I’ve been following since their earliest Kickstarter days. Matter of fact, the aforementioned power bank that’s currently sitting in a Chinese garbage dump is one of their products. R.I.P. noble battery pack.\n",
      "\n",
      "The Omni Mobile 12,800 mAh is a much more basic product than the company’s earliest offerings. There’s no display for power information here — instead you have to rely on four lights to let you know how much juice is left. There’s also a charging bad up top.\n",
      "\n",
      "As with most of the company’s products, I do quite like the design language. It’s subtle and unobtrusive and fits nicely inside a backpack. It’s definitely too big for carrying around in a pocket, however. Thanks to the wonders of USB, it will charge a laptop, as well, though once again, you’re going to run through that 12,800 mAh pretty quickly.\n",
      "\n",
      "The Fuse Chicken and OmniCharge run $85 and $99, respectively. They’ve both served me well as travel companions these last few weeks. Here’s to long flights and avoiding life’s landfill.\n"
     ]
    }
   ],
   "source": [
    "print(finance_related[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80d19513-706d-41d9-ae75-58fc101f70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the article, one can generate money by selling sustainable lifestyle products, such as shoes and apparel, leveraging a native mobile app to provide exclusive offers to loyal customers. This strategy offers a sustainable competitive advantage through the use of unique materials, such as tree fiber and merino wool, and promotes a strong brand identity centered around social and environmental values.\n",
      "\n",
      "The product offered is comfortable, sustainable, and high-quality footwear and apparel. The sustainable competitive advantage lies in the innovative use of natural materials and the emphasis on environmental stewardship. The potential profit can be generated through the following channels:\n",
      "\n",
      "* Selling products online and in-store\n",
      "* Offering exclusive deals to loyal customers through the native mobile app\n",
      "* Expanding into new markets and regions\n",
      "* Building a strong brand identity through social and environmental initiatives\n",
      "\n",
      "By focusing on sustainable products and promoting a strong brand identity, one can create a loyal customer base and generate revenue through sales, while also contributing to a more environmentally conscious market.\n"
     ]
    }
   ],
   "source": [
    "print(llm1.invoke({'input':finance_related[20]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b88f336-517f-4328-b04f-cb090db0f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = llm1.invoke({'input':finance_related[20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a3fcde-f4ac-4694-bf32-551e4ad7d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ac22138-cd49-49f4-9ff6-e132b6b74bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 131\n",
      "11 of 131\n",
      "21 of 131\n",
      "31 of 131\n",
      "41 of 131\n",
      "51 of 131\n",
      "61 of 131\n",
      "71 of 131\n",
      "81 of 131\n",
      "91 of 131\n",
      "101 of 131\n",
      "111 of 131\n",
      "121 of 131\n",
      "131 of 131\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(finance_related)):\n",
    "    if i%10 == 0:\n",
    "        print(f\"{i+1} of {len(finance_related)}\")\n",
    "    strategy = llm1.invoke({'input':finance_related[i]})\n",
    "    profile = llm2.invoke({'input':strategy})\n",
    "    full = f\"{strategy}\\n{profile}\"\n",
    "    strategy_list.append(full)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "959add41-8231-4fd0-a683-1df1140b0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('strategy_list.pkl', 'wb') as f:    # ‘wb’ = write in binary mode\n",
    "    pickle.dump(strategy_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed6821e-46ff-4717-9c8b-559657c52041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: 35\n",
      "Location: Sweden\n",
      "Highest Education Level: Bachelor's Degree\n",
      "Time Investment: 3 - 7 hrs\n",
      "Required Capital: $500 - $2000\n",
      "ROI Time Frame: 6 - 12 months\n"
     ]
    }
   ],
   "source": [
    "print(llm2.invoke({'input':strategy}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6243457c-30b3-4101-9d32-18916f8eef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "strategy_store = fa.from_texts(strategy_list, embedding=embedder)\n",
    "strategy_store.save_local(\"strategy_store_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c4912ea-e610-4738-aa8a-fe135fbdc8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy_store_idx/\n",
      "strategy_store_idx/index.faiss\n",
      "strategy_store_idx/index.pkl\n"
     ]
    }
   ],
   "source": [
    "!tar czvf strategy_store_idx.tgz strategy_store_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41dded76-01aa-4fd3-8a81-947a92168a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy_store_idx/\n",
      "strategy_store_idx/index.faiss\n",
      "strategy_store_idx/index.pkl\n"
     ]
    }
   ],
   "source": [
    "!tar xzvf strategy_store_idx.tgz\n",
    "new_db = fa.load_local(\"strategy_store_idx\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Domain Selling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc9409c0-0dc7-4ab4-9ffc-3fde34064042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the article, one can generate money by creating a company that provides a platform that connects businesses with existing marketplaces, offering a solution to optimize search, feed, and ads functions. The product can be a comprehensive advertising platform that uses artificial intelligence to provide a 360-degree view of market data, enabling marketplaces to predict conversion and clicks.\n",
      "\n",
      "The sustainable competitive advantage lies in the company's ability to integrate multiple functions under one umbrella, providing a seamless experience for marketplace customers. The potential profit comes from charging marketplaces a fee for using this platform, which can help increase sales and revenue for both the marketplace and the seller.\n",
      "\n",
      "To implement this strategy, one can start by identifying popular marketplaces and developing a platform that integrates search, feed, and ads functions. The platform can be built using artificial intelligence and data optimization capabilities to provide a 360-degree view of market data. The company can then partner with marketplaces to offer its platform, providing a solution that helps increase sales and revenue for both parties.\n",
      "Age: 35\n",
      "Location: United States\n",
      "Highest Education Level: Bachelor's Degree\n",
      "Time Investment: 7-20 hrs\n",
      "Required Capital: $500-$2000\n",
      "ROI Time Frame: 6-12 months\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560c736-09c1-48b3-9336-344e0a62e7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
